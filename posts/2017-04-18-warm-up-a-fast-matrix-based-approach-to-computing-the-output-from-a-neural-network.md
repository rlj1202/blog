---
layout:     post
title:      준비운동 뉴런 네트워크로 부터의 결과를 계산하기 위한 빠른 행렬 기반 접근방법
date:       2017-04-18 19:09:34 +0900
categories: 
tags:       NeuralNetworksAndDeepLearning
---
역전파를 언급하기 이전에, 뉴런 네트워크로 부터의 결과를 계산하기위한 빠른 행렬기반 알고리즘부터 시작해 봅시다. 사실 우리는 이미 전 장의 마지막 부분에서 이 알고리즘에 대해 요약적으로 보았습니다. 하지만 저는 이를 매우 빠르게 설명하였고 우리는 이 세부사항에 대해 다시 살펴볼 가치가 있습니다. 특히, 이는 비슷한 상황에서 역전파에 사용된 여러 기호들에 익숙해 지는 좋은 방법입니다.

모호한 방법으로 뉴런 네트워크의 가중치들을 언급하는 한 기호로 부터 시작해 봅시다. 우리는 $w^l_{jk}$라는 기호를 $l^{th}$ 층에 있는 $j^{th}$ 뉴런을 가리키는 $(l-1)^{th}$ 층에 있는 $k^{th}$ 뉴런의 연결을 나타내는데 사용할겁니다. 그래서, 예를 들면, 아래 다이어그램은 네트워크의 세번째 층에 있는 두번째 뉴런을 가리키는 두번째 층의 네번째 뉴런의 연결을 나타냅니다.

<center><img src="/assets/neuralnet/tikz16.png" style="max-width:100%;height:auto"  height="243" width="617"/></center>

<!-- more -->

이 기호는 처음에는 이해하기 어렵습니다. 그리고 이를 숙달하려면 좀 시간이 걸립니다. 하지만 조금만 노력하면 이 기호가 자연스럽고 쉽다고 느껴질 겁니다. 이 기호에서 별난점은 $j$와 $k$의 번호매김방법입니다. 여러분은 아마 $j$를 입력 뉴런으로, $k$를 출력 뉴런으로 생각하는것이 더 직관적이라고 생각할지도 모르겠습니다. 저는 이 별난점에 대한 이유를 아래 설명해 놓았습니다.

우리는 네트워크의 bias와 활성화에 대해 비슷한 기호를 사용합니다. 분명히, 우리는 $l^{th}$ 층에 있는 $j^{th}$ 뉴런의 bias 에 대해 $b^l_j$라고 표기합니다. 그리고 $l^{th}$ 층에 있는 $j^{th}$ 뉴런에의 활성화에 대해 $a^l_j$라는 기호를 사용합니다. 다음 그림은 이 기호들의 사용에 대한 예를 보여줍니다.

<center><img src="/assets/neuralnet/tikz17.png" style="max-width:100%;height:auto"  height="243" width="298"/></center>

이 기호들과 함께, $l^{th}$ 층의 $j^{th}$ 뉴런의 활성화 값은 다음 공식에 의해 $a^l_j$는 $(l-1)^{th}$층에 있는 활성화 값들과 관련이 있습니다. (이전 장에서의 4번 공식과 그에 대해 논한 것들과 함께 비교해 보세요)

$$\begin{eqnarray}   a^{l}_j = \sigma\left( \sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \right),\tag{23}\end{eqnarray}$$

시그마는 $(l-1)^{th}$층에 있는 모든 뉴런들에 대한 것 입니다. 이 식을 행렬의 형태로 다시 쓰기위해 우리는 각 층 $l$에 대해 가중치 행렬 $w^l$를 정의할겁니다. 가중치 행렬 $w^l$의 성분들은 단지 뉴런의 $l^{th}$층으로 연결된 가중치들을 의미합니다. 다시말하면, $j$번째 행과 $k$번째 열에 있는 성분은 $w^l_{jk}$입니다. 비슷하게, 각 $l$층에 대해 우리는 $bias$ 백터 $b^l$을 정의할 수 있습니다. 여러분은 아마 어떻게 이것이 작동할지 추측할 수 있습니다. bias 백터의 성분들은 단지 $b^l_j$ 의 값들입니다. 하나의 성분은 $l$번째 층의 각 뉴런에 대응됩니다. 그리고 마지막으로, 우리는 활성화 값 $a^l_j$들을 성분으로 가지는 활성화값 백터 $a^l$을 정의할 수 있습니다.

23번 공식을 행렬의 형태로 다시 쓰기 위해 우리에게 필요한 마지막 재료는 시그모이드 함수와 같은 함수를 백터화 하는것 입니다. 우리는 전 장에서 잠시 백터화에 대해 이미 만났지만 다시 요약하자면, 이 개념은 백터 $v$의 모든 성분에 시그모이드 함수와 같은 함수를 적용하는 것 입니다. 우리는 이러한 함수의 성분마다의 작업을 나타내기 위해 $\sigma (v)$라는 분명한 기호를 사용할 것 입니다. 다시 말하면, $\sigma (v)$의 각 성분은 단지 $\sigma (v)_j=\sigma (v_j)$입니다. 예를들면, 만약 우리가 $f(x)=x^2$라는 함수를 가지고 있다면 백터화된 형태의 함수 $f$는 다음과 같은 일을 할 것 입니다.

$$\begin{eqnarray}  f\left(\left[ \begin{array}{c} 2 \\ 3 \end{array} \right] \right)  = \left[ \begin{array}{c} f(2) \\ f(3) \end{array} \right]  = \left[ \begin{array}{c} 4 \\ 9 \end{array} \right],\tag{24}\end{eqnarray}$$

다시 말하면, 백터화된 함수 $f$는 백터의 모든 성분마다 제곱을 취하는 것 입니다.

이 기호들을 머릿속에 담아두면서, 23번 공식은 다음과 같이 아름답고 최소화된 백터화된 형태로 다시 쓰여질 수 있습니다.

$$\begin{eqnarray}   a^{l} = \sigma(w^l a^{l-1}+b^l).\tag{25}\end{eqnarray}$$

이 식은 우리에게 어떻게 한 층에서의 활성화가 이전의 층에서의 활성화값들과 관계가 있는지에 대해 생각할 수 있는 좀 더 전체적인 방법을 말해줍니다: 우리는 단지 활성화 값들에 가중치 행렬을 곱하고, bias 백터를 더하고, 그리고 최종적으로 시그모이드 함수를 적용하였습니다. 이런 전체적인 시각은 우리가 지금까지 취해왔던 뉴런단위의 시각보다 쉽고 더 간단명로 합니다.(그리고 더 적은 번호들을 사용합니다!) 무엇이 일어나고 있는지에 대해 명확히 하면서 이를 번호매김의 지옥에서 벗어나는 한 방법으로 생각해 보세요. 이 식은 대부분의 행렬 라이브러리가 행렬곱과 백터 덧셈의 빠른 구현을 제공하고 있기 떄문에 현실적으로도 굉장히 유용합니다. 실제로, 전 장에서의 코드는 네트워크의 행동을 계산하기 위해 이러한 식의 사용을 내포하고 있었습니다.

$a^l$을 계산하기 위해 25번 식을 사용할때, 우리는 이 방법을 따라 $z^l \equiv w^la^{l-1}+b^l$ 라는 중간 값을 계산합니다. 이 값은 이름을 지어주기에 충분히 쓸모있습니다: 우리는 이 $z^l$라는 값을 $l$번째 레이어의 뉴런들에 대한 weighted input(입력 가중치)이라고 부릅니다. 우리는 이 장의 후반부에서 입력 가중치 $z^l$의 많은 사용을 하게 될 겁니다. 25번 식은 입력 가중치에 대해서 $a^l=\sigma (z^l)$와 같이 쓰여지곤 합니다. $z^l$는 $z^l_j=\sum_k w^l_{jk}a^{l-1}_k+b^l_j$라는 항들을 가지고 있다고 알려드리는것이 좋겠군요. 다시 말하면, $z^l_j$는 단지 $l$번째 층의 $j$번째 뉴런에 대한 활성화 함수로의 입력 가중치 입니다.

