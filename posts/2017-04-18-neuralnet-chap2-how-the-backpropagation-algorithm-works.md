---
layout:     post
title:      제 2장 - 역전파 알고리즘이 어떻게 작동하는가
date:       2017-04-18 18:55:08 +0900
categories: 
tags:       NeuralNetworksAndDeepLearning
---

전 장에서 우리는 어떻게 뉴런 네트워크가 기울기 하강 알고리즘을 사용해서 가중치와 $bias$를 학습하는지에 대해 보았습니다. 그러나 여기에서 우리의 설명에는 큰 구멍이 하나 있었습니다: 우리는 어떻게 비용함수의 기울기를 계산하는지 이야기 하지 않았습니다. 정말 큰 구멍입니다! 이 장에서 저는 역전파 라고 알려진 그런 기울기를 계산하는 빠른 알고리즘에 대해서 설명할 것 입니다.

역전파 알고리즘은 1970년대에 처음 소개되었지만, [데이비드 루멜하트][david_rumelhart], [제프리 힌튼][hinton], 그리고 [로널드 윌리엄스][ronald_williams]의 [유명한 1986년도의 논문][1986_paper]이 나오기 전 까지는 그 중요성이 인정되지 않았습니다. 이 논문은 이전에는 풀리지 않았던 문제들을 풀 수 있는 뉴런 네트워크의 사용을 가능케 하면서 이전의 학습에 대한 접근방법보다 역전파 알고리즘이 더 빨리 작동하는 몇개의 뉴런 네트워크에 대해 설명하였습니다. 오늘날, 역전파 알고리즘은 뉴런 네트워크에서의 학습의 대표주자가 되었습니다.

<!-- more -->

이 장은 이 책의 나머지 부분보다 수학적인 부분이 많이 연관되어 있습니다. 여러분이 수학에 미치지 않았다면 이 장을 넘겨버리고 역전파를 무시해버리고 싶은 내용들을 담은 판도라 상자처럼 여기고 싶을겁니다. 이런 내용들을 공부하는데 왜 시간을 투자해야 할까요?

그 이유는, 당연히, 이해를 위한겁니다. 역전파의 핵심은 네트워크에서의 그 어떤 가중치 $w$(또는 bias $b$)에 대한 비용 함수 $C$의 편미분 $\partial C/\partial w$의 식입니다. 이 식은 우리에게 우리가 가중치와 $bias$를 바꿀때 비용함수가 얼마나 빠르게 바뀌는지에 대해 말해줍니다. 그리고 이 식이 좀 복잡한 반면, 각 항들은 자연적이고 직관적이 해석이 가능한 아름다움을 가지고 있습니다. 그리고 또한 역전파는 학습을 위한 단지 빠른 알고리즘이 아닙니다. 이는 가중치와 $bias$의 변화가 네트워크의 전체 행동을 변화시키는지에 대한 자세한 식견을 알려줍니다. 자세한 사항을 공부하는것은 정말 가치가 있습니다.

앞서 말했듯이, 이 장을 넘기고 슥 지나가고 싶거나 그냥 바로 다음장으로 넘어가고 싶다면, 괜찮습니다. 저는 여러분이 역전파를 알수없는 판도라의 상자와 같이 다루더라도 책의 나머지 부분은 접근하기 쉽도록 작성하였으니까요. 물론 이 장으로 부터의 결론을 다시 언급하게될&nbsp;책의 후반부에는 중요한 요점들이 있습니다. 그러나 그 중요한 요점들에서는 여러분이 모든 이유를 따라가지 못하더라도 주 결론에 대한 이해를 할 수 있어야 합니다.

[david_rumelhart]: https://en.wikipedia.org/wiki/David_Rumelhart
[hinton]: https://www.cs.toronto.edu/~hinton/
[ronald_williams]: https://en.wikipedia.org/wiki/Ronald_J._Williams
[1986_paper]: https://www.nature.com/nature/journal/v323/n6088/pdf/323533a0.pdf
