<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0" class="jsx-834064900"/><link rel="icon" href="/blog/favicon.ico" class="jsx-834064900"/><title class="jsx-1900133614">뉴런 네트워크와 딥 러닝</title><meta name="next-head-count" content="4"/><link rel="preload" href="/blog/_next/static/D0XTK0MEKRJDXLhC_InxX/pages/_app.js" as="script"/><link rel="preload" href="/blog/_next/static/D0XTK0MEKRJDXLhC_InxX/pages/blog/%5B...slug%5D.js" as="script"/><link rel="preload" href="/blog/_next/static/runtime/webpack-c212667a5f965e81e004.js" as="script"/><link rel="preload" href="/blog/_next/static/chunks/framework.619a4f70c1d4d3a29cbc.js" as="script"/><link rel="preload" href="/blog/_next/static/chunks/commons.020a96d8a8e71e9e3362.js" as="script"/><link rel="preload" href="/blog/_next/static/runtime/main-d76a4f4aadf0225115f4.js" as="script"/><link rel="preload" href="/blog/_next/static/chunks/e6dc419047dac61ef10415aaccc4c72c1149af9c.5637e4aab0992574c455.js" as="script"/><style id="__jsx-505471054">.top.jsx-1900133614{margin:0;background-color:#ff6565;box-shadow:0 0 5pt black;color:white;}.fixed-width.jsx-1900133614{max-width:600pt;min-width:0;padding:0 20pt;margin:0 auto;}.header.jsx-1900133614{padding-top:20pt;padding-bottom:20pt;}.header.jsx-1900133614 .navigator.jsx-1900133614{margin-bottom:20pt;}.header.jsx-1900133614 .navigator-link.jsx-1900133614{font-size:8pt;color:black;background-color:white;border-radius:16pt;padding:2pt 6pt;}.header.jsx-1900133614 .date.jsx-1900133614{margin:10pt 0;}.header.jsx-1900133614 .title.jsx-1900133614{font-size:35pt;font-weight:bold;margin:10pt 0;}.header.jsx-1900133614 .subtitle.jsx-1900133614{font-weight:bold;margin:10pt 0;}.tags.jsx-1900133614{margin-top:10pt;}.tag.jsx-1900133614{margin-right:4pt;padding:2pt 4pt;font-size:9pt;border-radius:3pt;background-color:#0000007a;display:inline-block;}.content-wrapper.jsx-1900133614{margin:20pt 0;}hr.jsx-1900133614{border:0;height:1px;background-color:#00000014;margin:20pt 0;}.prev-post.jsx-1900133614 .heading.jsx-1900133614,.next-post.jsx-1900133614 .heading.jsx-1900133614{margin-bottom:5pt;}.prev-post.jsx-1900133614 .title.jsx-1900133614,.next-post.jsx-1900133614 .title.jsx-1900133614{font-style:italic;color:#7d7d7d;}.next-post.jsx-1900133614{text-align:right;}.footer.jsx-1900133614{text-align:center;font-size:9pt;margin:20pt 0;}</style><style id="__jsx-1919476713">code{background-color:#dddddd;font-size:inherit;padding:2pt 4pt;border-radius:2pt;}.content img,.content iframe{max-width:100%;}</style><style id="__jsx-834064900">@import url(https://fonts.googleapis.com/earlyaccess/notosanskr.css);html,body{margin:0;padding:0;font-family:'Noto Sans KR',sans-serif;}a{color:inherit;-webkit-text-decoration:none;text-decoration:none;}*{box-sizing:border-box;}</style></head><body><div id="__next"><div class="jsx-834064900 container"><div class="jsx-1900133614 top"><div class="jsx-1900133614 header fixed-width"><div class="jsx-1900133614 navigator"><span class="jsx-1900133614 navigator-link"><a class="jsx-1900133614" href="/blog/">home</a></span> &gt;<span class="jsx-1900133614 navigator-link">blog</span></div><div class="jsx-1900133614 date">2017-04-08 20:44:00 +0900</div><div class="jsx-1900133614 title"><a class="jsx-1900133614" href="/blog/blog/2017-04-08-neural-network-and-deep-learning">뉴런 네트워크와 딥 러닝</a></div><div class="jsx-1900133614 subtitle"></div><div class="jsx-1900133614 tags"><span class="jsx-1900133614 tag">#<!-- -->NeuralNetworksAndDeepLearning</span></div></div></div><div class="jsx-1900133614 content-wrapper"><div class="jsx-1900133614 content fixed-width"><p>이 글의 원문은 <a href="https://neuralnetworksanddeeplearning.com">https://neuralnetworksanddeeplearning.com</a> 입니다. 이 번역 글은 이 책의 저작자표시-비영리 라이센스를 따릅니다. 또한 이 글 또한 같은 라이센스를 따릅니다.</p><p>개인적인 공부(인공지능과 영어)와 정보 공유를 위해 번역을 시작하게 되었으며, 번역과 영어에 미숙하기 때문에 여러 오역등이 있을 수 있으며 오역, 오타 등알 발견했을 때 댓글로 알려주시면 수정하도록 하겠습니다.</p><span><!-- more --></span><ul><li><a href="">이 책이 다루고 있는 것</a></li><li><a href="">제 1장 - 손으로 쓴 숫자를 인식하는 뉴런 네트워크 사용하기</a><ul><li><a href="">퍼셉트론</a></li><li><a href="">시그모이드 뉴런</a></li><li><a href="">뉴런 네트워크의 구조</a></li><li><a href="">손으로 쓴 숫자를 구분하기 위한 간단한 네트워크</a></li><li><a href="">기울기 하강 알고리즘으로 학습</a></li><li><a href="">숫자들을 판별하기 위한 네트워크 구현하기</a></li><li><a href="">딥러닝을 향해</a></li></ul></li><li><a href="">제 2장 - 역전파 알고리즘이 어떻게 작동하는가</a><ul><li><a href="">준비운동: 뉴런 네트워크로 부터의 결과를 계산하기 위한 빠른 행렬 기반 접근 방법</a></li><li><a href="">비용함수에 대해 필요한 두 가지 추정</a></li><li><a href="">하다마드 곱</a></li><li><a href="">역전파에 대한 네 가지 중요한 공식</a></li></ul></li><li><a href="">제 3장 - 뉴런 네트워크가 학습하는 방법 향상하기</a><ul><li>교차 엔트로피 비용 함수</li><li>과적합과 일반화</li><li>가중치 초기화</li><li>손글씨 인식문제 다시보기: 코드</li><li>뉴런 네트워크의 hyper-parameter를 어떻게 선택하는가?</li><li>다른 기술들</li></ul></li><li>제 4장 - 뉴런 네트워크가 어떤 함수라도 계산 가능하다는 시각적 증명<ul><li>Two caveats</li><li>Univerality with one input and one output</li><li>Many input variables</li><li>Extension beyond sigmoid neurons</li><li>Fixing up the step functions</li><li>Conclusion</li></ul></li><li>제 5장 - 왜 딥 뉴런 네트워크는 학습하기 어려운가?<ul><li>The vanising gradient problem</li><li>What&#x27;s causing the vanishing gradient problem? Unstable gradients in deep neural nets</li><li>Unstable gradients in more complex networks</li><li>Other obstacles to deep learning</li></ul></li><li>제 6장 - 딥 러닝<ul><li>Introducing convolutional networks</li><li>Convolutional neural networks in practice</li><li>The code for our convolutional networks</li><li>Recent progress in image recognition</li><li>Other approaches to deep neural nets</li><li>On the future of neural networks</li></ul></li><li>Appendix: Is there a simple algorithm for intelligence?</li><li>Acknowledgements</li><li>Frequently Asdked Questions</li></ul><p>아래서 부터는 해당 온라인 책의 내용입니다.</p><hr/><p>뉴런 네트워크와 딥 러닝은 온라인으로 읽을 수 있는 무료 책 입니다. 이 책에서는 다음과 같은것을 배울 수 있습니다.</p><ul><li>뉴런 네트워크, 관측 가능한 데이터로 부터 학습하는 컴퓨터를 가능하게 하는 아름다운 생물학적 프로그래밍 패러다임</li><li>딥 러닝, 뉴런 네트워크의 학습을 위한 강력한 기술</li></ul><p>뉴런 네트워크와 딥 러닝은 현재 이미지 인식, 음성 인식, 자연어 처리 등의 많은 문제들의 가장 좋은 해결책을 제공하고 있습니다. 이 책은 뉴런 네트워크와 딥 러닝에 있어서 가장 핵심적인 개념들을 여러분에게 알려줄 것입니다.</p><p>이 책에서 사용되는 접근법에 대해서 더 자세히 알고싶다면, <a href="">여기</a>를 참고하세요. 또는 바로 <a href="">제 1장</a>으로 건너가 시작할 수 있습니다.</p><p><a href="">book_about</a>: {{ site.baseurl }}{% post_url 2017-04-08-what-this-book-is-about %}
<a href="">chap1</a>:      {{ site.baseurl }}{% post_url 2017-04-08-neuralnet-chap1-using-neural-nets-to-recognize-handwritten-digits %}
<a href="">perceptrons</a>: {{ site.baseurl }}{% post_url 2017-04-08-perceptrons %}
<a href="">sigmoid</a>: {{ site.baseurl }}{% post_url 2017-04-08-sigmoid-neurons %}
<a href="">architecture</a>: {{ site.baseurl }}{% post_url 2017-04-09-the-architecture-of-neural-networks %}
<a href="">simple_network</a>: {{ site.baseurl }}{% post_url 2017-04-09-a-simple-network-to-classify-handwritten-digits %}
<a href="">gradient_descent</a>: {{ site.baseurl }}{% post_url 2017-04-11-learning-with-gradient-descent %}
<a href="">implementing_network</a>: {{ site.baseurl }}{% post_url 2017-04-15-implementing-our-network-to-classify-digits %}
<a href="">toward_deep_learning</a>: {{ site.baseurl }}{% post_url 2017-04-18-toward-deep-learning %}
<a href="">chap2</a>: {{ site.baseurl }}{% post_url 2017-04-18-neuralnet-chap2-how-the-backpropagation-algorithm-works %}
<a href="">warm_up</a>: {{ site.baseurl }}{% post_url 2017-04-18-warm-up-a-fast-matrix-based-approach-to-computing-the-output-from-a-neural-network %}
<a href="">two_assumptions</a>: {{ site.baseurl }}{% post_url 2017-04-18-the-two-assumptions-we-need-about-the-cost-function %}
<a href="">hadamard</a>: {{ site.baseurl }}{% post_url 2017-04-18-the-hadamard-product %}
<a href="">four_equations</a>: {{ site.baseurl }}{% post_url 2017-04-18-the-four-fundamental-equations-behind-backpropagation %}
<a href="">chap3</a>: {{ site.baseurl }}{% post_url 2017-05-14-neuralnet-chap3-improving-the-way-neural-networks-learn %}</p><hr class="jsx-1900133614"/><div class="jsx-1900133614 prev-post"><h2 class="jsx-1900133614 heading">이전글</h2><span class="jsx-1900133614 title">&quot;The test title&quot;</span></div><div class="jsx-1900133614 next-post"><h2 class="jsx-1900133614 heading">다음글</h2><span class="jsx-1900133614 title">&quot;Another post title&quot;</span></div><hr class="jsx-1900133614"/><footer class="jsx-1900133614 footer">Github.io - Copyright(c) 2020. Jisu Sim(RedLaboratory)</footer></div></div><style>
            .container {
            }
            </style></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"slug":["2017-04-08-neural-network-and-deep-learning"],"frontmatter":"{\"layout\":\"post\",\"title\":\"뉴런 네트워크와 딥 러닝\",\"date\":\"2017-04-08 20:44:00 +0900\",\"categories\":null,\"tags\":\"NeuralNetworksAndDeepLearning\"}","markdownbody":"\n이 글의 원문은 [https://neuralnetworksanddeeplearning.com](https://neuralnetworksanddeeplearning.com) 입니다. 이 번역 글은 이 책의 저작자표시-비영리 라이센스를 따릅니다. 또한 이 글 또한 같은 라이센스를 따릅니다.\n\n개인적인 공부(인공지능과 영어)와 정보 공유를 위해 번역을 시작하게 되었으며, 번역과 영어에 미숙하기 때문에 여러 오역등이 있을 수 있으며 오역, 오타 등알 발견했을 때 댓글로 알려주시면 수정하도록 하겠습니다.\n\n\u003c!-- more --\u003e\n\n* [이 책이 다루고 있는 것][book_about]\n* [제 1장 - 손으로 쓴 숫자를 인식하는 뉴런 네트워크 사용하기][chap1]\n  * [퍼셉트론][perceptrons]\n  * [시그모이드 뉴런][sigmoid]\n  * [뉴런 네트워크의 구조][architecture]\n  * [손으로 쓴 숫자를 구분하기 위한 간단한 네트워크][simple_network]\n  * [기울기 하강 알고리즘으로 학습][gradient_descent]\n  * [숫자들을 판별하기 위한 네트워크 구현하기][implementing_network]\n  * [딥러닝을 향해][toward_deep_learning]\n* [제 2장 - 역전파 알고리즘이 어떻게 작동하는가][chap2]\n  * [준비운동: 뉴런 네트워크로 부터의 결과를 계산하기 위한 빠른 행렬 기반 접근 방법][warm_up]\n  * [비용함수에 대해 필요한 두 가지 추정][two_assumptions]\n  * [하다마드 곱][hadamard]\n  * [역전파에 대한 네 가지 중요한 공식][four_equations]\n* [제 3장 - 뉴런 네트워크가 학습하는 방법 향상하기][chap3]\n  * 교차 엔트로피 비용 함수\n  * 과적합과 일반화\n  * 가중치 초기화\n  * 손글씨 인식문제 다시보기: 코드\n  * 뉴런 네트워크의 hyper-parameter를 어떻게 선택하는가?\n  * 다른 기술들\n* 제 4장 - 뉴런 네트워크가 어떤 함수라도 계산 가능하다는 시각적 증명\n  * Two caveats\n  * Univerality with one input and one output\n  * Many input variables\n  * Extension beyond sigmoid neurons\n  * Fixing up the step functions\n  * Conclusion\n* 제 5장 - 왜 딥 뉴런 네트워크는 학습하기 어려운가?\n  * The vanising gradient problem\n  * What's causing the vanishing gradient problem? Unstable gradients in deep neural nets\n  * Unstable gradients in more complex networks\n  * Other obstacles to deep learning\n* 제 6장 - 딥 러닝\n  * Introducing convolutional networks\n  * Convolutional neural networks in practice\n  * The code for our convolutional networks\n  * Recent progress in image recognition\n  * Other approaches to deep neural nets\n  * On the future of neural networks\n* Appendix: Is there a simple algorithm for intelligence?\n* Acknowledgements\n* Frequently Asdked Questions\n\n아래서 부터는 해당 온라인 책의 내용입니다.\n\n------\n\n뉴런 네트워크와 딥 러닝은 온라인으로 읽을 수 있는 무료 책 입니다. 이 책에서는 다음과 같은것을 배울 수 있습니다.\n\n* 뉴런 네트워크, 관측 가능한 데이터로 부터 학습하는 컴퓨터를 가능하게 하는 아름다운 생물학적 프로그래밍 패러다임\n* 딥 러닝, 뉴런 네트워크의 학습을 위한 강력한 기술\n\n뉴런 네트워크와 딥 러닝은 현재 이미지 인식, 음성 인식, 자연어 처리 등의 많은 문제들의 가장 좋은 해결책을 제공하고 있습니다. 이 책은 뉴런 네트워크와 딥 러닝에 있어서 가장 핵심적인 개념들을 여러분에게 알려줄 것입니다.\n\n이 책에서 사용되는 접근법에 대해서 더 자세히 알고싶다면, [여기][book_about]를 참고하세요. 또는 바로 [제 1장][chap1]으로 건너가 시작할 수 있습니다.\n\n[book_about]: {{ site.baseurl }}{% post_url 2017-04-08-what-this-book-is-about %}\n[chap1]:      {{ site.baseurl }}{% post_url 2017-04-08-neuralnet-chap1-using-neural-nets-to-recognize-handwritten-digits %}\n[perceptrons]: {{ site.baseurl }}{% post_url 2017-04-08-perceptrons %}\n[sigmoid]: {{ site.baseurl }}{% post_url 2017-04-08-sigmoid-neurons %}\n[architecture]: {{ site.baseurl }}{% post_url 2017-04-09-the-architecture-of-neural-networks %}\n[simple_network]: {{ site.baseurl }}{% post_url 2017-04-09-a-simple-network-to-classify-handwritten-digits %}\n[gradient_descent]: {{ site.baseurl }}{% post_url 2017-04-11-learning-with-gradient-descent %}\n[implementing_network]: {{ site.baseurl }}{% post_url 2017-04-15-implementing-our-network-to-classify-digits %}\n[toward_deep_learning]: {{ site.baseurl }}{% post_url 2017-04-18-toward-deep-learning %}\n[chap2]: {{ site.baseurl }}{% post_url 2017-04-18-neuralnet-chap2-how-the-backpropagation-algorithm-works %}\n[warm_up]: {{ site.baseurl }}{% post_url 2017-04-18-warm-up-a-fast-matrix-based-approach-to-computing-the-output-from-a-neural-network %}\n[two_assumptions]: {{ site.baseurl }}{% post_url 2017-04-18-the-two-assumptions-we-need-about-the-cost-function %}\n[hadamard]: {{ site.baseurl }}{% post_url 2017-04-18-the-hadamard-product %}\n[four_equations]: {{ site.baseurl }}{% post_url 2017-04-18-the-four-fundamental-equations-behind-backpropagation %}\n[chap3]: {{ site.baseurl }}{% post_url 2017-05-14-neuralnet-chap3-improving-the-way-neural-networks-learn %}\n"},"__N_SSG":true},"page":"/blog/[...slug]","query":{"slug":["2017-04-08-neural-network-and-deep-learning"]},"buildId":"D0XTK0MEKRJDXLhC_InxX","assetPrefix":"/blog","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/blog/_next/static/runtime/polyfills-b10afcedf826ebd862ad.js"></script><script async="" data-next-page="/_app" src="/blog/_next/static/D0XTK0MEKRJDXLhC_InxX/pages/_app.js"></script><script async="" data-next-page="/blog/[...slug]" src="/blog/_next/static/D0XTK0MEKRJDXLhC_InxX/pages/blog/%5B...slug%5D.js"></script><script src="/blog/_next/static/runtime/webpack-c212667a5f965e81e004.js" async=""></script><script src="/blog/_next/static/chunks/framework.619a4f70c1d4d3a29cbc.js" async=""></script><script src="/blog/_next/static/chunks/commons.020a96d8a8e71e9e3362.js" async=""></script><script src="/blog/_next/static/runtime/main-d76a4f4aadf0225115f4.js" async=""></script><script src="/blog/_next/static/chunks/e6dc419047dac61ef10415aaccc4c72c1149af9c.5637e4aab0992574c455.js" async=""></script><script src="/blog/_next/static/D0XTK0MEKRJDXLhC_InxX/_buildManifest.js" async=""></script><script src="/blog/_next/static/D0XTK0MEKRJDXLhC_InxX/_ssgManifest.js" async=""></script></body></html>