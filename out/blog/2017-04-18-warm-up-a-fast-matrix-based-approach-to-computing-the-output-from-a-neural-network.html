<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0" class="jsx-834064900"/><link rel="icon" href="/blog/favicon.ico" class="jsx-834064900"/><title class="jsx-4179923229">준비운동 뉴런 네트워크로 부터의 결과를 계산하기 위한 빠른 행렬 기반 접근방법</title><meta name="next-head-count" content="4"/><link rel="preload" href="/blog/_next/static/HQZBoNW2ycimZLlBm8Rpq/pages/_app.js" as="script"/><link rel="preload" href="/blog/_next/static/HQZBoNW2ycimZLlBm8Rpq/pages/blog/%5B...slug%5D.js" as="script"/><link rel="preload" href="/blog/_next/static/runtime/webpack-c212667a5f965e81e004.js" as="script"/><link rel="preload" href="/blog/_next/static/chunks/framework.619a4f70c1d4d3a29cbc.js" as="script"/><link rel="preload" href="/blog/_next/static/chunks/commons.020a96d8a8e71e9e3362.js" as="script"/><link rel="preload" href="/blog/_next/static/runtime/main-d76a4f4aadf0225115f4.js" as="script"/><link rel="preload" href="/blog/_next/static/chunks/93913148a803e02e94a06c77fa5869838dbed1a1.5637e4aab0992574c455.js" as="script"/><style id="__jsx-275247549">.top.jsx-4179923229{margin:0;background-color:#ff6565;box-shadow:0 0 5pt black;color:white;}.fixed-width.jsx-4179923229{max-width:600pt;min-width:0;padding:0 20pt;margin:0 auto;}.header.jsx-4179923229{padding-top:20pt;padding-bottom:20pt;}.header.jsx-4179923229 .date.jsx-4179923229{margin-bottom:10pt;}.header.jsx-4179923229 .title.jsx-4179923229{font-size:35pt;font-weight:bold;margin:10pt 0;}.header.jsx-4179923229 .subtitle.jsx-4179923229{font-weight:bold;margin:10pt 0;}.tags.jsx-4179923229{margin-top:10pt;}.tag.jsx-4179923229{margin-right:4pt;padding:2pt 4pt;font-size:9pt;border-radius:3pt;background-color:#0000007a;display:inline-block;}.content-wrapper.jsx-4179923229{margin:20pt 0;}</style><style id="__jsx-1919476713">code{background-color:#dddddd;font-size:inherit;padding:2pt 4pt;border-radius:2pt;}.content img,.content iframe{max-width:100%;}</style><style id="__jsx-834064900">@import url(https://fonts.googleapis.com/earlyaccess/notosanskr.css);html,body{margin:0;padding:0;font-family:'Noto Sans KR',sans-serif;}a{color:inherit;-webkit-text-decoration:none;text-decoration:none;}*{box-sizing:border-box;}</style></head><body><div id="__next"><div class="jsx-834064900 container"><div class="jsx-4179923229 top"><div class="jsx-4179923229 header fixed-width"><div class="jsx-4179923229 date">2017-04-18 19:09:34 +0900</div><div class="jsx-4179923229 title"><a class="jsx-4179923229" href="/blog/blog/2017-04-18-warm-up-a-fast-matrix-based-approach-to-computing-the-output-from-a-neural-network">준비운동 뉴런 네트워크로 부터의 결과를 계산하기 위한 빠른 행렬 기반 접근방법</a></div><div class="jsx-4179923229 subtitle"></div><div class="jsx-4179923229 tags"><span class="jsx-4179923229 tag">#<!-- -->NeuralNetworksAndDeepLearning</span></div></div></div><div class="jsx-4179923229 content-wrapper"><div class="jsx-4179923229 content fixed-width"><p>역전파를 언급하기 이전에, 뉴런 네트워크로 부터의 결과를 계산하기위한 빠른 행렬기반 알고리즘부터 시작해 봅시다. 사실 우리는 이미 전 장의 마지막 부분에서 이 알고리즘에 대해 요약적으로 보았습니다. 하지만 저는 이를 매우 빠르게 설명하였고 우리는 이 세부사항에 대해 다시 살펴볼 가치가 있습니다. 특히, 이는 비슷한 상황에서 역전파에 사용된 여러 기호들에 익숙해 지는 좋은 방법입니다.</p><p>모호한 방법으로 뉴런 네트워크의 가중치들을 언급하는 한 기호로 부터 시작해 봅시다. 우리는 $w^l_{jk}$라는 기호를 $l^{th}$ 층에 있는 $j^{th}$ 뉴런을 가리키는 $(l-1)^{th}$ 층에 있는 $k^{th}$ 뉴런의 연결을 나타내는데 사용할겁니다. 그래서, 예를 들면, 아래 다이어그램은 네트워크의 세번째 층에 있는 두번째 뉴런을 가리키는 두번째 층의 네번째 뉴런의 연결을 나타냅니다.</p><span><center><img src="/assets/neuralnet/tikz16.png" style="max-width:100%;height:auto"  height="243" width="617"/></center></span><span><!-- more --></span><p>이 기호는 처음에는 이해하기 어렵습니다. 그리고 이를 숙달하려면 좀 시간이 걸립니다. 하지만 조금만 노력하면 이 기호가 자연스럽고 쉽다고 느껴질 겁니다. 이 기호에서 별난점은 $j$와 $k$의 번호매김방법입니다. 여러분은 아마 $j$를 입력 뉴런으로, $k$를 출력 뉴런으로 생각하는것이 더 직관적이라고 생각할지도 모르겠습니다. 저는 이 별난점에 대한 이유를 아래 설명해 놓았습니다.</p><p>우리는 네트워크의 bias와 활성화에 대해 비슷한 기호를 사용합니다. 분명히, 우리는 $l^{th}$ 층에 있는 $j^{th}$ 뉴런의 bias 에 대해 $b^l_j$라고 표기합니다. 그리고 $l^{th}$ 층에 있는 $j^{th}$ 뉴런에의 활성화에 대해 $a^l_j$라는 기호를 사용합니다. 다음 그림은 이 기호들의 사용에 대한 예를 보여줍니다.</p><span><center><img src="/assets/neuralnet/tikz17.png" style="max-width:100%;height:auto"  height="243" width="298"/></center></span><p>이 기호들과 함께, $l^{th}$ 층의 $j^{th}$ 뉴런의 활성화 값은 다음 공식에 의해 $a^l_j$는 $(l-1)^{th}$층에 있는 활성화 값들과 관련이 있습니다. (이전 장에서의 4번 공식과 그에 대해 논한 것들과 함께 비교해 보세요)</p><p>$$\begin{eqnarray}   a^{l}<em>j = \sigma\left( \sum_k w^{l}</em>{jk} a^{l-1}_k + b^l_j \right),\tag{23}\end{eqnarray}$$</p><p>시그마는 $(l-1)^{th}$층에 있는 모든 뉴런들에 대한 것 입니다. 이 식을 행렬의 형태로 다시 쓰기위해 우리는 각 층 $l$에 대해 가중치 행렬 $w^l$를 정의할겁니다. 가중치 행렬 $w^l$의 성분들은 단지 뉴런의 $l^{th}$층으로 연결된 가중치들을 의미합니다. 다시말하면, $j$번째 행과 $k$번째 열에 있는 성분은 $w^l_{jk}$입니다. 비슷하게, 각 $l$층에 대해 우리는 $bias$ 백터 $b^l$을 정의할 수 있습니다. 여러분은 아마 어떻게 이것이 작동할지 추측할 수 있습니다. bias 백터의 성분들은 단지 $b^l_j$ 의 값들입니다. 하나의 성분은 $l$번째 층의 각 뉴런에 대응됩니다. 그리고 마지막으로, 우리는 활성화 값 $a^l_j$들을 성분으로 가지는 활성화값 백터 $a^l$을 정의할 수 있습니다.</p><p>23번 공식을 행렬의 형태로 다시 쓰기 위해 우리에게 필요한 마지막 재료는 시그모이드 함수와 같은 함수를 백터화 하는것 입니다. 우리는 전 장에서 잠시 백터화에 대해 이미 만났지만 다시 요약하자면, 이 개념은 백터 $v$의 모든 성분에 시그모이드 함수와 같은 함수를 적용하는 것 입니다. 우리는 이러한 함수의 성분마다의 작업을 나타내기 위해 $\sigma (v)$라는 분명한 기호를 사용할 것 입니다. 다시 말하면, $\sigma (v)$의 각 성분은 단지 $\sigma (v)_j=\sigma (v_j)$입니다. 예를들면, 만약 우리가 $f(x)=x^2$라는 함수를 가지고 있다면 백터화된 형태의 함수 $f$는 다음과 같은 일을 할 것 입니다.</p><p>$$\begin{eqnarray}  f\left(\left<a href=""> \begin{array}{c} 2 <!-- -->\<!-- --> 3 \end{array} \right</a> \right)  = \left<a href=""> \begin{array}{c} f(2) <!-- -->\<!-- --> f(3) \end{array} \right</a>  = \left<a href=""> \begin{array}{c} 4 <!-- -->\<!-- --> 9 \end{array} \right</a>,\tag{24}\end{eqnarray}$$</p><p>다시 말하면, 백터화된 함수 $f$는 백터의 모든 성분마다 제곱을 취하는 것 입니다.</p><p>이 기호들을 머릿속에 담아두면서, 23번 공식은 다음과 같이 아름답고 최소화된 백터화된 형태로 다시 쓰여질 수 있습니다.</p><p>$$\begin{eqnarray}   a^{l} = \sigma(w^l a^{l-1}+b^l).\tag{25}\end{eqnarray}$$</p><p>이 식은 우리에게 어떻게 한 층에서의 활성화가 이전의 층에서의 활성화값들과 관계가 있는지에 대해 생각할 수 있는 좀 더 전체적인 방법을 말해줍니다: 우리는 단지 활성화 값들에 가중치 행렬을 곱하고, bias 백터를 더하고, 그리고 최종적으로 시그모이드 함수를 적용하였습니다. 이런 전체적인 시각은 우리가 지금까지 취해왔던 뉴런단위의 시각보다 쉽고 더 간단명로 합니다.(그리고 더 적은 번호들을 사용합니다!) 무엇이 일어나고 있는지에 대해 명확히 하면서 이를 번호매김의 지옥에서 벗어나는 한 방법으로 생각해 보세요. 이 식은 대부분의 행렬 라이브러리가 행렬곱과 백터 덧셈의 빠른 구현을 제공하고 있기 떄문에 현실적으로도 굉장히 유용합니다. 실제로, 전 장에서의 코드는 네트워크의 행동을 계산하기 위해 이러한 식의 사용을 내포하고 있었습니다.</p><p>$a^l$을 계산하기 위해 25번 식을 사용할때, 우리는 이 방법을 따라 $z^l \equiv w^la^{l-1}+b^l$ 라는 중간 값을 계산합니다. 이 값은 이름을 지어주기에 충분히 쓸모있습니다: 우리는 이 $z^l$라는 값을 $l$번째 레이어의 뉴런들에 대한 weighted input(입력 가중치)이라고 부릅니다. 우리는 이 장의 후반부에서 입력 가중치 $z^l$의 많은 사용을 하게 될 겁니다. 25번 식은 입력 가중치에 대해서 $a^l=\sigma (z^l)$와 같이 쓰여지곤 합니다. $z^l$는 $z^l<em>j=\sum_k w^l</em>{jk}a^{l-1}_k+b^l_j$라는 항들을 가지고 있다고 알려드리는것이 좋겠군요. 다시 말하면, $z^l_j$는 단지 $l$번째 층의 $j$번째 뉴런에 대한 활성화 함수로의 입력 가중치 입니다.</p></div></div><style>
            .container {
            }
            </style></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"slug":["2017-04-18-warm-up-a-fast-matrix-based-approach-to-computing-the-output-from-a-neural-network"],"frontmatter":"{\"layout\":\"post\",\"title\":\"준비운동 뉴런 네트워크로 부터의 결과를 계산하기 위한 빠른 행렬 기반 접근방법\",\"date\":\"2017-04-18 19:09:34 +0900\",\"categories\":null,\"tags\":\"NeuralNetworksAndDeepLearning\"}","markdownbody":"역전파를 언급하기 이전에, 뉴런 네트워크로 부터의 결과를 계산하기위한 빠른 행렬기반 알고리즘부터 시작해 봅시다. 사실 우리는 이미 전 장의 마지막 부분에서 이 알고리즘에 대해 요약적으로 보았습니다. 하지만 저는 이를 매우 빠르게 설명하였고 우리는 이 세부사항에 대해 다시 살펴볼 가치가 있습니다. 특히, 이는 비슷한 상황에서 역전파에 사용된 여러 기호들에 익숙해 지는 좋은 방법입니다.\n\n모호한 방법으로 뉴런 네트워크의 가중치들을 언급하는 한 기호로 부터 시작해 봅시다. 우리는 $w^l_{jk}$라는 기호를 $l^{th}$ 층에 있는 $j^{th}$ 뉴런을 가리키는 $(l-1)^{th}$ 층에 있는 $k^{th}$ 뉴런의 연결을 나타내는데 사용할겁니다. 그래서, 예를 들면, 아래 다이어그램은 네트워크의 세번째 층에 있는 두번째 뉴런을 가리키는 두번째 층의 네번째 뉴런의 연결을 나타냅니다.\n\n\u003ccenter\u003e\u003cimg src=\"/assets/neuralnet/tikz16.png\" style=\"max-width:100%;height:auto\"  height=\"243\" width=\"617\"/\u003e\u003c/center\u003e\n\n\u003c!-- more --\u003e\n\n이 기호는 처음에는 이해하기 어렵습니다. 그리고 이를 숙달하려면 좀 시간이 걸립니다. 하지만 조금만 노력하면 이 기호가 자연스럽고 쉽다고 느껴질 겁니다. 이 기호에서 별난점은 $j$와 $k$의 번호매김방법입니다. 여러분은 아마 $j$를 입력 뉴런으로, $k$를 출력 뉴런으로 생각하는것이 더 직관적이라고 생각할지도 모르겠습니다. 저는 이 별난점에 대한 이유를 아래 설명해 놓았습니다.\n\n우리는 네트워크의 bias와 활성화에 대해 비슷한 기호를 사용합니다. 분명히, 우리는 $l^{th}$ 층에 있는 $j^{th}$ 뉴런의 bias 에 대해 $b^l_j$라고 표기합니다. 그리고 $l^{th}$ 층에 있는 $j^{th}$ 뉴런에의 활성화에 대해 $a^l_j$라는 기호를 사용합니다. 다음 그림은 이 기호들의 사용에 대한 예를 보여줍니다.\n\n\u003ccenter\u003e\u003cimg src=\"/assets/neuralnet/tikz17.png\" style=\"max-width:100%;height:auto\"  height=\"243\" width=\"298\"/\u003e\u003c/center\u003e\n\n이 기호들과 함께, $l^{th}$ 층의 $j^{th}$ 뉴런의 활성화 값은 다음 공식에 의해 $a^l_j$는 $(l-1)^{th}$층에 있는 활성화 값들과 관련이 있습니다. (이전 장에서의 4번 공식과 그에 대해 논한 것들과 함께 비교해 보세요)\n\n$$\\begin{eqnarray}   a^{l}_j = \\sigma\\left( \\sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \\right),\\tag{23}\\end{eqnarray}$$\n\n시그마는 $(l-1)^{th}$층에 있는 모든 뉴런들에 대한 것 입니다. 이 식을 행렬의 형태로 다시 쓰기위해 우리는 각 층 $l$에 대해 가중치 행렬 $w^l$를 정의할겁니다. 가중치 행렬 $w^l$의 성분들은 단지 뉴런의 $l^{th}$층으로 연결된 가중치들을 의미합니다. 다시말하면, $j$번째 행과 $k$번째 열에 있는 성분은 $w^l_{jk}$입니다. 비슷하게, 각 $l$층에 대해 우리는 $bias$ 백터 $b^l$을 정의할 수 있습니다. 여러분은 아마 어떻게 이것이 작동할지 추측할 수 있습니다. bias 백터의 성분들은 단지 $b^l_j$ 의 값들입니다. 하나의 성분은 $l$번째 층의 각 뉴런에 대응됩니다. 그리고 마지막으로, 우리는 활성화 값 $a^l_j$들을 성분으로 가지는 활성화값 백터 $a^l$을 정의할 수 있습니다.\n\n23번 공식을 행렬의 형태로 다시 쓰기 위해 우리에게 필요한 마지막 재료는 시그모이드 함수와 같은 함수를 백터화 하는것 입니다. 우리는 전 장에서 잠시 백터화에 대해 이미 만났지만 다시 요약하자면, 이 개념은 백터 $v$의 모든 성분에 시그모이드 함수와 같은 함수를 적용하는 것 입니다. 우리는 이러한 함수의 성분마다의 작업을 나타내기 위해 $\\sigma (v)$라는 분명한 기호를 사용할 것 입니다. 다시 말하면, $\\sigma (v)$의 각 성분은 단지 $\\sigma (v)_j=\\sigma (v_j)$입니다. 예를들면, 만약 우리가 $f(x)=x^2$라는 함수를 가지고 있다면 백터화된 형태의 함수 $f$는 다음과 같은 일을 할 것 입니다.\n\n$$\\begin{eqnarray}  f\\left(\\left[ \\begin{array}{c} 2 \\\\ 3 \\end{array} \\right] \\right)  = \\left[ \\begin{array}{c} f(2) \\\\ f(3) \\end{array} \\right]  = \\left[ \\begin{array}{c} 4 \\\\ 9 \\end{array} \\right],\\tag{24}\\end{eqnarray}$$\n\n다시 말하면, 백터화된 함수 $f$는 백터의 모든 성분마다 제곱을 취하는 것 입니다.\n\n이 기호들을 머릿속에 담아두면서, 23번 공식은 다음과 같이 아름답고 최소화된 백터화된 형태로 다시 쓰여질 수 있습니다.\n\n$$\\begin{eqnarray}   a^{l} = \\sigma(w^l a^{l-1}+b^l).\\tag{25}\\end{eqnarray}$$\n\n이 식은 우리에게 어떻게 한 층에서의 활성화가 이전의 층에서의 활성화값들과 관계가 있는지에 대해 생각할 수 있는 좀 더 전체적인 방법을 말해줍니다: 우리는 단지 활성화 값들에 가중치 행렬을 곱하고, bias 백터를 더하고, 그리고 최종적으로 시그모이드 함수를 적용하였습니다. 이런 전체적인 시각은 우리가 지금까지 취해왔던 뉴런단위의 시각보다 쉽고 더 간단명로 합니다.(그리고 더 적은 번호들을 사용합니다!) 무엇이 일어나고 있는지에 대해 명확히 하면서 이를 번호매김의 지옥에서 벗어나는 한 방법으로 생각해 보세요. 이 식은 대부분의 행렬 라이브러리가 행렬곱과 백터 덧셈의 빠른 구현을 제공하고 있기 떄문에 현실적으로도 굉장히 유용합니다. 실제로, 전 장에서의 코드는 네트워크의 행동을 계산하기 위해 이러한 식의 사용을 내포하고 있었습니다.\n\n$a^l$을 계산하기 위해 25번 식을 사용할때, 우리는 이 방법을 따라 $z^l \\equiv w^la^{l-1}+b^l$ 라는 중간 값을 계산합니다. 이 값은 이름을 지어주기에 충분히 쓸모있습니다: 우리는 이 $z^l$라는 값을 $l$번째 레이어의 뉴런들에 대한 weighted input(입력 가중치)이라고 부릅니다. 우리는 이 장의 후반부에서 입력 가중치 $z^l$의 많은 사용을 하게 될 겁니다. 25번 식은 입력 가중치에 대해서 $a^l=\\sigma (z^l)$와 같이 쓰여지곤 합니다. $z^l$는 $z^l_j=\\sum_k w^l_{jk}a^{l-1}_k+b^l_j$라는 항들을 가지고 있다고 알려드리는것이 좋겠군요. 다시 말하면, $z^l_j$는 단지 $l$번째 층의 $j$번째 뉴런에 대한 활성화 함수로의 입력 가중치 입니다.\n\n"},"__N_SSG":true},"page":"/blog/[...slug]","query":{"slug":["2017-04-18-warm-up-a-fast-matrix-based-approach-to-computing-the-output-from-a-neural-network"]},"buildId":"HQZBoNW2ycimZLlBm8Rpq","assetPrefix":"/blog","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/blog/_next/static/runtime/polyfills-b10afcedf826ebd862ad.js"></script><script async="" data-next-page="/_app" src="/blog/_next/static/HQZBoNW2ycimZLlBm8Rpq/pages/_app.js"></script><script async="" data-next-page="/blog/[...slug]" src="/blog/_next/static/HQZBoNW2ycimZLlBm8Rpq/pages/blog/%5B...slug%5D.js"></script><script src="/blog/_next/static/runtime/webpack-c212667a5f965e81e004.js" async=""></script><script src="/blog/_next/static/chunks/framework.619a4f70c1d4d3a29cbc.js" async=""></script><script src="/blog/_next/static/chunks/commons.020a96d8a8e71e9e3362.js" async=""></script><script src="/blog/_next/static/runtime/main-d76a4f4aadf0225115f4.js" async=""></script><script src="/blog/_next/static/chunks/93913148a803e02e94a06c77fa5869838dbed1a1.5637e4aab0992574c455.js" async=""></script><script src="/blog/_next/static/HQZBoNW2ycimZLlBm8Rpq/_buildManifest.js" async=""></script><script src="/blog/_next/static/HQZBoNW2ycimZLlBm8Rpq/_ssgManifest.js" async=""></script></body></html>