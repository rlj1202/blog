<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0" class="jsx-834064900"/><link rel="icon" href="/blog/favicon.ico" class="jsx-834064900"/><title class="jsx-1900133614">뉴런 네트워크의 구조</title><meta name="next-head-count" content="4"/><link rel="preload" href="/blog/_next/static/D0XTK0MEKRJDXLhC_InxX/pages/_app.js" as="script"/><link rel="preload" href="/blog/_next/static/D0XTK0MEKRJDXLhC_InxX/pages/blog/%5B...slug%5D.js" as="script"/><link rel="preload" href="/blog/_next/static/runtime/webpack-c212667a5f965e81e004.js" as="script"/><link rel="preload" href="/blog/_next/static/chunks/framework.619a4f70c1d4d3a29cbc.js" as="script"/><link rel="preload" href="/blog/_next/static/chunks/commons.020a96d8a8e71e9e3362.js" as="script"/><link rel="preload" href="/blog/_next/static/runtime/main-d76a4f4aadf0225115f4.js" as="script"/><link rel="preload" href="/blog/_next/static/chunks/e6dc419047dac61ef10415aaccc4c72c1149af9c.5637e4aab0992574c455.js" as="script"/><style id="__jsx-505471054">.top.jsx-1900133614{margin:0;background-color:#ff6565;box-shadow:0 0 5pt black;color:white;}.fixed-width.jsx-1900133614{max-width:600pt;min-width:0;padding:0 20pt;margin:0 auto;}.header.jsx-1900133614{padding-top:20pt;padding-bottom:20pt;}.header.jsx-1900133614 .navigator.jsx-1900133614{margin-bottom:20pt;}.header.jsx-1900133614 .navigator-link.jsx-1900133614{font-size:8pt;color:black;background-color:white;border-radius:16pt;padding:2pt 6pt;}.header.jsx-1900133614 .date.jsx-1900133614{margin:10pt 0;}.header.jsx-1900133614 .title.jsx-1900133614{font-size:35pt;font-weight:bold;margin:10pt 0;}.header.jsx-1900133614 .subtitle.jsx-1900133614{font-weight:bold;margin:10pt 0;}.tags.jsx-1900133614{margin-top:10pt;}.tag.jsx-1900133614{margin-right:4pt;padding:2pt 4pt;font-size:9pt;border-radius:3pt;background-color:#0000007a;display:inline-block;}.content-wrapper.jsx-1900133614{margin:20pt 0;}hr.jsx-1900133614{border:0;height:1px;background-color:#00000014;margin:20pt 0;}.prev-post.jsx-1900133614 .heading.jsx-1900133614,.next-post.jsx-1900133614 .heading.jsx-1900133614{margin-bottom:5pt;}.prev-post.jsx-1900133614 .title.jsx-1900133614,.next-post.jsx-1900133614 .title.jsx-1900133614{font-style:italic;color:#7d7d7d;}.next-post.jsx-1900133614{text-align:right;}.footer.jsx-1900133614{text-align:center;font-size:9pt;margin:20pt 0;}</style><style id="__jsx-1919476713">code{background-color:#dddddd;font-size:inherit;padding:2pt 4pt;border-radius:2pt;}.content img,.content iframe{max-width:100%;}</style><style id="__jsx-834064900">@import url(https://fonts.googleapis.com/earlyaccess/notosanskr.css);html,body{margin:0;padding:0;font-family:'Noto Sans KR',sans-serif;}a{color:inherit;-webkit-text-decoration:none;text-decoration:none;}*{box-sizing:border-box;}</style></head><body><div id="__next"><div class="jsx-834064900 container"><div class="jsx-1900133614 top"><div class="jsx-1900133614 header fixed-width"><div class="jsx-1900133614 navigator"><span class="jsx-1900133614 navigator-link"><a class="jsx-1900133614" href="/blog/">home</a></span> &gt;<span class="jsx-1900133614 navigator-link">blog</span></div><div class="jsx-1900133614 date">2017-04-09 13:50:37 +0900</div><div class="jsx-1900133614 title"><a class="jsx-1900133614" href="/blog/blog/2017-04-09-the-architecture-of-neural-networks">뉴런 네트워크의 구조</a></div><div class="jsx-1900133614 subtitle"></div><div class="jsx-1900133614 tags"><span class="jsx-1900133614 tag">#<!-- -->NeuralNetworksAndDeepLearning</span></div></div></div><div class="jsx-1900133614 content-wrapper"><div class="jsx-1900133614 content fixed-width"><p>다음 차례에서는 손글씨를 꽤 잘 인식할 수 있는 뉴런 네트워크를 소개해 드릴겁니다. 그 전에, 네트워크의 각 부분을 부르는 몇 개의 전문 용어들을 설명하는 것이 도움이 되겠군요. 우리가 아래와 같은 네트워크를 가지고 있다고 해봅시다.</p><span><center><img src="/assets/neuralnet/tikz10.png" style="max-width:100%;height:auto"  height="211" style="" width="396"/></center></span><p>앞서 언급했듯이, 가장 왼쪽에 위치한 층은 입력 층이라고 불리고, 그 안에 있는 뉴런들은 입력 뉴런이라고 불립니다. 가장 오른쪽에 있는 출력층은 출력 뉴런들을 가지고 있습니다. 위 예시에서는 하나의 뉴런만을 가지고 있군요. 가운데 있는 층은 입력 뉴런 또는 출력 뉴런을 가지고 있지 않으므로 은닉층이라고 불립니다. &quot;은닉&quot;이라는 용어는 이상하게 들릴 수 있습니다. (제가 처음 이 단어를 들었을때는 이 은닉층이 꽤 깊은 철학적 의미 또는 수학적 중요성을 가지고 있는줄 알았습니다.) 하지만 은닉층은 정말 &quot;입력 또는 출력이 아님&quot; 이라는 의미 그 이상, 그 이하도 아닌 그 자체를 의미합니다. 위에 있는 네트워크는 단 하나의 은닉 레이어만을 가지고 있지만 몇몇 네트워크들은 여러개의 은닉층을 가지고 있습니다. 예를 들면, 다음의 4층 네트워크는 두개의 은닉층을 가지고 있습니다.</p><span><!-- more --></span><span><center><img src="/assets/neuralnet/tikz11.png" style="max-width:100%;height:auto"  height="324" style="" width="597"/></center></span><p>혼란스럽게도, 어떤 역사적인 이유로 인해 다층 네트워크들은 시그모이드 뉴런으로 이루어져 있음에도 불구하고 다층 퍼셉트론 또는 MLPs(Multi Layer Percptrons)라고 불립니다. 저는 이 책에서 MLP라는 전문용어를 사용하진 않을테지만, 여러분에게 이런 용어가 있음을 알려드리고 싶었습니다.</p><p>네트워크에서 이런 입력과 출력층의 구조는 일반적으로 일방통행적입니다. 예를 들어, 우리가 손글씨 이미지가 &quot;9&quot;를 나타내는지 아닌지를 결정하려고 한다고 합시다. 이 네트워크를 디자인 하는 일반적 방법은 입력층에 들어갈 이미지의 픽셀들의 명도를 변환하는것 입니다. 이미지의 크기가 $64\times 64$ 인 흑백 이미지라면, 우리는 $4096=64\times 64$개의 입력 뉴런들을 가지게 되고, 명도값들은 0과 1 사이의 값을 가지게 됩니다. 출력층은 단 한개의 출력뉴런만을 가지게 되고 0.5 보다 작은 값은 &quot;이 이미지는 9가 아닙니다&quot;를, 0.5보다 큰 값은 &quot;이 이미지는 9 입니다&quot;를 나타내게 됩니다.</p><p>뉴런 네트워크의 입력과 출력층들의 구조가 일반적으로 일방통행적 이지만, 은닉층의 디자인에 꽤나 예술적인 면이 있을 수 있습니다. 이러한 상황에서는, 단지 몇개의 간단한 규칙만으로 은칙층의 계산과정을 줄일 수 없습니다. 대신, 뉴런 네트워크 연구자들은 네트워크에서 이끌어 내고자 하는 행동을 얻게 도와주는 은닉층을 위한 많은 간편한(heuristic, 어림짐작의) 구조를 개발해 냈습니다. 예를 들면, 어떤 간편한 방법은 어떻게 네트워크를 학습시키기 위해 필요한 시간에 대해 은닉층의 수를 결정할 것인지 도와줍니다. 우리는 나중에 이 책에서 몇개의 경험적 방법들을 배워볼 겁니다.</p><p>지금까지 우리는 한 층의 출력이 다음 레이어의 입력으로써 쓰이는, 일방통행적 뉴런 네트워크에 대해서 이야기 해 왔습니다. 이런 네트워크들은 feedforward 뉴런 네트워크라고 불리는데, 이는 네트워크에 어떠한 재귀도 존재하지 않음을 의미합니다. 정보는 항상 앞으로 이동합니다. 절대로 뒤로 영향을 주지 않습니다. 만약 우리가 재귀를 가지게 된다면, 우리는 시그모이드 함수로 들어가는 입력값에 따라 출력값이 달라지는 상황과는 다른 상황을 맞이하게 될 것입니다. 이에 대한 개념을 배우기는 어려우니, 우리는 그러한 재귀를 만들지 않을 것 입니다.</p><p>그러나, 뒤로 영향을 주는 재귀가 가능한 가상 뉴런 네트워크의 다른 모델들 또한 존재합니다. 이런 모델들은 <span><a href="https://en.wikipedia.org/wiki/Recurrent_neural_network" target="_blank" class="tx-link"></span>회귀적 뉴런 네트워크<span></a></span>라고 불립니다. 이러한 모델의 개념은 활성화 된 상태가 끝나기 전에 일정시간 동안 발화하는 뉴런이 있다는 것 입니다. 이러한 발화는 다른 뉴런들에 영향을 주어서 일정시간 간격을 두고 일정시간 동안 발화하게 할 수 있습니다. 이는 또한 더 많은 뉴런들을 발화하게 할 수 있고, 시간이 지남에 따라 계속 진행되는 뉴런의 발화를 볼 수 있을 것 입니다. 루프는 이러한 모델에서 뉴런의 출력은 즉시 이루어 지지 않고 일정 시간 간격을 두고 자신의 입력에만 영향을 미치기 때문에 어떠한 문제도 일으키지 않습니다.</p><p>회귀적 뉴런 네트워크는 학습 알고리즘이 다른 네트워크 모델보다 강력하지 않기 때문에 feedforward 네트워크 보다는 적은 영향력을 행사하고 있습니다. 하지만 이런 회귀적 네트워크는 여전히 흥미로운 모델입니다. 이는 우리의 뇌가 작동하는 방법과 feedforward 네트워크보다 비슷합니다. 또한 회귀적 네트워크는 feedforward 네트워크로는 굉장히 어렵게 풀어야 하는 문제들을 풀 수 있습니다. 그러나, 우리의 능력의 한계로 인해, 이 책에서는 더 자주 사용되는 feedforward 네트워크에만 집중 하려고 합니다.</p><hr class="jsx-1900133614"/><div class="jsx-1900133614 prev-post"><h2 class="jsx-1900133614 heading">이전글</h2><span class="jsx-1900133614 title">&quot;The test title&quot;</span></div><div class="jsx-1900133614 next-post"><h2 class="jsx-1900133614 heading">다음글</h2><span class="jsx-1900133614 title">&quot;Another post title&quot;</span></div><hr class="jsx-1900133614"/><footer class="jsx-1900133614 footer">Github.io - Copyright(c) 2020. Jisu Sim(RedLaboratory)</footer></div></div><style>
            .container {
            }
            </style></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"slug":["2017-04-09-the-architecture-of-neural-networks"],"frontmatter":"{\"layout\":\"post\",\"title\":\"뉴런 네트워크의 구조\",\"date\":\"2017-04-09 13:50:37 +0900\",\"categories\":null,\"tags\":\"NeuralNetworksAndDeepLearning\"}","markdownbody":"\n다음 차례에서는 손글씨를 꽤 잘 인식할 수 있는 뉴런 네트워크를 소개해 드릴겁니다. 그 전에, 네트워크의 각 부분을 부르는 몇 개의 전문 용어들을 설명하는 것이 도움이 되겠군요. 우리가 아래와 같은 네트워크를 가지고 있다고 해봅시다.\n\n\u003ccenter\u003e\u003cimg src=\"/assets/neuralnet/tikz10.png\" style=\"max-width:100%;height:auto\"  height=\"211\" style=\"\" width=\"396\"/\u003e\u003c/center\u003e\n\n앞서 언급했듯이, 가장 왼쪽에 위치한 층은 입력 층이라고 불리고, 그 안에 있는 뉴런들은 입력 뉴런이라고 불립니다. 가장 오른쪽에 있는 출력층은 출력 뉴런들을 가지고 있습니다. 위 예시에서는 하나의 뉴런만을 가지고 있군요. 가운데 있는 층은 입력 뉴런 또는 출력 뉴런을 가지고 있지 않으므로 은닉층이라고 불립니다. \"은닉\"이라는 용어는 이상하게 들릴 수 있습니다. (제가 처음 이 단어를 들었을때는 이 은닉층이 꽤 깊은 철학적 의미 또는 수학적 중요성을 가지고 있는줄 알았습니다.) 하지만 은닉층은 정말 \"입력 또는 출력이 아님\" 이라는 의미 그 이상, 그 이하도 아닌 그 자체를 의미합니다. 위에 있는 네트워크는 단 하나의 은닉 레이어만을 가지고 있지만 몇몇 네트워크들은 여러개의 은닉층을 가지고 있습니다. 예를 들면, 다음의 4층 네트워크는 두개의 은닉층을 가지고 있습니다.\n\n\u003c!-- more --\u003e\n\n\u003ccenter\u003e\u003cimg src=\"/assets/neuralnet/tikz11.png\" style=\"max-width:100%;height:auto\"  height=\"324\" style=\"\" width=\"597\"/\u003e\u003c/center\u003e\n\n혼란스럽게도, 어떤 역사적인 이유로 인해 다층 네트워크들은 시그모이드 뉴런으로 이루어져 있음에도 불구하고 다층 퍼셉트론 또는 MLPs(Multi Layer Percptrons)라고 불립니다. 저는 이 책에서 MLP라는 전문용어를 사용하진 않을테지만, 여러분에게 이런 용어가 있음을 알려드리고 싶었습니다.\n\n네트워크에서 이런 입력과 출력층의 구조는 일반적으로 일방통행적입니다. 예를 들어, 우리가 손글씨 이미지가 \"9\"를 나타내는지 아닌지를 결정하려고 한다고 합시다. 이 네트워크를 디자인 하는 일반적 방법은 입력층에 들어갈 이미지의 픽셀들의 명도를 변환하는것 입니다. 이미지의 크기가 $64\\times 64$ 인 흑백 이미지라면, 우리는 $4096=64\\times 64$개의 입력 뉴런들을 가지게 되고, 명도값들은 0과 1 사이의 값을 가지게 됩니다. 출력층은 단 한개의 출력뉴런만을 가지게 되고 0.5 보다 작은 값은 \"이 이미지는 9가 아닙니다\"를, 0.5보다 큰 값은 \"이 이미지는 9 입니다\"를 나타내게 됩니다.\n\n뉴런 네트워크의 입력과 출력층들의 구조가 일반적으로 일방통행적 이지만, 은닉층의 디자인에 꽤나 예술적인 면이 있을 수 있습니다. 이러한 상황에서는, 단지 몇개의 간단한 규칙만으로 은칙층의 계산과정을 줄일 수 없습니다. 대신, 뉴런 네트워크 연구자들은 네트워크에서 이끌어 내고자 하는 행동을 얻게 도와주는 은닉층을 위한 많은 간편한(heuristic, 어림짐작의) 구조를 개발해 냈습니다. 예를 들면, 어떤 간편한 방법은 어떻게 네트워크를 학습시키기 위해 필요한 시간에 대해 은닉층의 수를 결정할 것인지 도와줍니다. 우리는 나중에 이 책에서 몇개의 경험적 방법들을 배워볼 겁니다.\n\n지금까지 우리는 한 층의 출력이 다음 레이어의 입력으로써 쓰이는, 일방통행적 뉴런 네트워크에 대해서 이야기 해 왔습니다. 이런 네트워크들은 feedforward 뉴런 네트워크라고 불리는데, 이는 네트워크에 어떠한 재귀도 존재하지 않음을 의미합니다. 정보는 항상 앞으로 이동합니다. 절대로 뒤로 영향을 주지 않습니다. 만약 우리가 재귀를 가지게 된다면, 우리는 시그모이드 함수로 들어가는 입력값에 따라 출력값이 달라지는 상황과는 다른 상황을 맞이하게 될 것입니다. 이에 대한 개념을 배우기는 어려우니, 우리는 그러한 재귀를 만들지 않을 것 입니다.\n\n그러나, 뒤로 영향을 주는 재귀가 가능한 가상 뉴런 네트워크의 다른 모델들 또한 존재합니다. 이런 모델들은 \u003ca href=\"https://en.wikipedia.org/wiki/Recurrent_neural_network\" target=\"_blank\" class=\"tx-link\"\u003e회귀적 뉴런 네트워크\u003c/a\u003e라고 불립니다. 이러한 모델의 개념은 활성화 된 상태가 끝나기 전에 일정시간 동안 발화하는 뉴런이 있다는 것 입니다. 이러한 발화는 다른 뉴런들에 영향을 주어서 일정시간 간격을 두고 일정시간 동안 발화하게 할 수 있습니다. 이는 또한 더 많은 뉴런들을 발화하게 할 수 있고, 시간이 지남에 따라 계속 진행되는 뉴런의 발화를 볼 수 있을 것 입니다. 루프는 이러한 모델에서 뉴런의 출력은 즉시 이루어 지지 않고 일정 시간 간격을 두고 자신의 입력에만 영향을 미치기 때문에 어떠한 문제도 일으키지 않습니다.\n\n회귀적 뉴런 네트워크는 학습 알고리즘이 다른 네트워크 모델보다 강력하지 않기 때문에 feedforward 네트워크 보다는 적은 영향력을 행사하고 있습니다. 하지만 이런 회귀적 네트워크는 여전히 흥미로운 모델입니다. 이는 우리의 뇌가 작동하는 방법과 feedforward 네트워크보다 비슷합니다. 또한 회귀적 네트워크는 feedforward 네트워크로는 굉장히 어렵게 풀어야 하는 문제들을 풀 수 있습니다. 그러나, 우리의 능력의 한계로 인해, 이 책에서는 더 자주 사용되는 feedforward 네트워크에만 집중 하려고 합니다.\n\n"},"__N_SSG":true},"page":"/blog/[...slug]","query":{"slug":["2017-04-09-the-architecture-of-neural-networks"]},"buildId":"D0XTK0MEKRJDXLhC_InxX","assetPrefix":"/blog","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/blog/_next/static/runtime/polyfills-b10afcedf826ebd862ad.js"></script><script async="" data-next-page="/_app" src="/blog/_next/static/D0XTK0MEKRJDXLhC_InxX/pages/_app.js"></script><script async="" data-next-page="/blog/[...slug]" src="/blog/_next/static/D0XTK0MEKRJDXLhC_InxX/pages/blog/%5B...slug%5D.js"></script><script src="/blog/_next/static/runtime/webpack-c212667a5f965e81e004.js" async=""></script><script src="/blog/_next/static/chunks/framework.619a4f70c1d4d3a29cbc.js" async=""></script><script src="/blog/_next/static/chunks/commons.020a96d8a8e71e9e3362.js" async=""></script><script src="/blog/_next/static/runtime/main-d76a4f4aadf0225115f4.js" async=""></script><script src="/blog/_next/static/chunks/e6dc419047dac61ef10415aaccc4c72c1149af9c.5637e4aab0992574c455.js" async=""></script><script src="/blog/_next/static/D0XTK0MEKRJDXLhC_InxX/_buildManifest.js" async=""></script><script src="/blog/_next/static/D0XTK0MEKRJDXLhC_InxX/_ssgManifest.js" async=""></script></body></html>