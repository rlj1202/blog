<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0" class="jsx-834064900"/><link rel="icon" href="/blog/favicon.ico" class="jsx-834064900"/><title class="jsx-4179923229">제 2장 - 역전파 알고리즘이 어떻게 작동하는가</title><meta name="next-head-count" content="4"/><link rel="preload" href="/blog/_next/static/HQZBoNW2ycimZLlBm8Rpq/pages/_app.js" as="script"/><link rel="preload" href="/blog/_next/static/HQZBoNW2ycimZLlBm8Rpq/pages/blog/%5B...slug%5D.js" as="script"/><link rel="preload" href="/blog/_next/static/runtime/webpack-c212667a5f965e81e004.js" as="script"/><link rel="preload" href="/blog/_next/static/chunks/framework.619a4f70c1d4d3a29cbc.js" as="script"/><link rel="preload" href="/blog/_next/static/chunks/commons.020a96d8a8e71e9e3362.js" as="script"/><link rel="preload" href="/blog/_next/static/runtime/main-d76a4f4aadf0225115f4.js" as="script"/><link rel="preload" href="/blog/_next/static/chunks/93913148a803e02e94a06c77fa5869838dbed1a1.5637e4aab0992574c455.js" as="script"/><style id="__jsx-275247549">.top.jsx-4179923229{margin:0;background-color:#ff6565;box-shadow:0 0 5pt black;color:white;}.fixed-width.jsx-4179923229{max-width:600pt;min-width:0;padding:0 20pt;margin:0 auto;}.header.jsx-4179923229{padding-top:20pt;padding-bottom:20pt;}.header.jsx-4179923229 .date.jsx-4179923229{margin-bottom:10pt;}.header.jsx-4179923229 .title.jsx-4179923229{font-size:35pt;font-weight:bold;margin:10pt 0;}.header.jsx-4179923229 .subtitle.jsx-4179923229{font-weight:bold;margin:10pt 0;}.tags.jsx-4179923229{margin-top:10pt;}.tag.jsx-4179923229{margin-right:4pt;padding:2pt 4pt;font-size:9pt;border-radius:3pt;background-color:#0000007a;display:inline-block;}.content-wrapper.jsx-4179923229{margin:20pt 0;}</style><style id="__jsx-1919476713">code{background-color:#dddddd;font-size:inherit;padding:2pt 4pt;border-radius:2pt;}.content img,.content iframe{max-width:100%;}</style><style id="__jsx-834064900">@import url(https://fonts.googleapis.com/earlyaccess/notosanskr.css);html,body{margin:0;padding:0;font-family:'Noto Sans KR',sans-serif;}a{color:inherit;-webkit-text-decoration:none;text-decoration:none;}*{box-sizing:border-box;}</style></head><body><div id="__next"><div class="jsx-834064900 container"><div class="jsx-4179923229 top"><div class="jsx-4179923229 header fixed-width"><div class="jsx-4179923229 date">2017-04-18 18:55:08 +0900</div><div class="jsx-4179923229 title"><a class="jsx-4179923229" href="/blog/blog/2017-04-18-neuralnet-chap2-how-the-backpropagation-algorithm-works">제 2장 - 역전파 알고리즘이 어떻게 작동하는가</a></div><div class="jsx-4179923229 subtitle"></div><div class="jsx-4179923229 tags"><span class="jsx-4179923229 tag">#<!-- -->NeuralNetworksAndDeepLearning</span></div></div></div><div class="jsx-4179923229 content-wrapper"><div class="jsx-4179923229 content fixed-width"><p>전 장에서 우리는 어떻게 뉴런 네트워크가 기울기 하강 알고리즘을 사용해서 가중치와 $bias$를 학습하는지에 대해 보았습니다. 그러나 여기에서 우리의 설명에는 큰 구멍이 하나 있었습니다: 우리는 어떻게 비용함수의 기울기를 계산하는지 이야기 하지 않았습니다. 정말 큰 구멍입니다! 이 장에서 저는 역전파 라고 알려진 그런 기울기를 계산하는 빠른 알고리즘에 대해서 설명할 것 입니다.</p><p>역전파 알고리즘은 1970년대에 처음 소개되었지만, <a href="https://en.wikipedia.org/wiki/David_Rumelhart">데이비드 루멜하트</a>, <a href="https://www.cs.toronto.edu/~hinton/">제프리 힌튼</a>, 그리고 <a href="https://en.wikipedia.org/wiki/Ronald_J._Williams">로널드 윌리엄스</a>의 <a href="https://www.nature.com/nature/journal/v323/n6088/pdf/323533a0.pdf">유명한 1986년도의 논문</a>이 나오기 전 까지는 그 중요성이 인정되지 않았습니다. 이 논문은 이전에는 풀리지 않았던 문제들을 풀 수 있는 뉴런 네트워크의 사용을 가능케 하면서 이전의 학습에 대한 접근방법보다 역전파 알고리즘이 더 빨리 작동하는 몇개의 뉴런 네트워크에 대해 설명하였습니다. 오늘날, 역전파 알고리즘은 뉴런 네트워크에서의 학습의 대표주자가 되었습니다.</p><span><!-- more --></span><p>이 장은 이 책의 나머지 부분보다 수학적인 부분이 많이 연관되어 있습니다. 여러분이 수학에 미치지 않았다면 이 장을 넘겨버리고 역전파를 무시해버리고 싶은 내용들을 담은 판도라 상자처럼 여기고 싶을겁니다. 이런 내용들을 공부하는데 왜 시간을 투자해야 할까요?</p><p>그 이유는, 당연히, 이해를 위한겁니다. 역전파의 핵심은 네트워크에서의 그 어떤 가중치 $w$(또는 bias $b$)에 대한 비용 함수 $C$의 편미분 $\partial C/\partial w$의 식입니다. 이 식은 우리에게 우리가 가중치와 $bias$를 바꿀때 비용함수가 얼마나 빠르게 바뀌는지에 대해 말해줍니다. 그리고 이 식이 좀 복잡한 반면, 각 항들은 자연적이고 직관적이 해석이 가능한 아름다움을 가지고 있습니다. 그리고 또한 역전파는 학습을 위한 단지 빠른 알고리즘이 아닙니다. 이는 가중치와 $bias$의 변화가 네트워크의 전체 행동을 변화시키는지에 대한 자세한 식견을 알려줍니다. 자세한 사항을 공부하는것은 정말 가치가 있습니다.</p><p>앞서 말했듯이, 이 장을 넘기고 슥 지나가고 싶거나 그냥 바로 다음장으로 넘어가고 싶다면, 괜찮습니다. 저는 여러분이 역전파를 알수없는 판도라의 상자와 같이 다루더라도 책의 나머지 부분은 접근하기 쉽도록 작성하였으니까요. 물론 이 장으로 부터의 결론을 다시 언급하게될<!-- --> <!-- -->책의 후반부에는 중요한 요점들이 있습니다. 그러나 그 중요한 요점들에서는 여러분이 모든 이유를 따라가지 못하더라도 주 결론에 대한 이해를 할 수 있어야 합니다.</p></div></div><style>
            .container {
            }
            </style></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"slug":["2017-04-18-neuralnet-chap2-how-the-backpropagation-algorithm-works"],"frontmatter":"{\"layout\":\"post\",\"title\":\"제 2장 - 역전파 알고리즘이 어떻게 작동하는가\",\"date\":\"2017-04-18 18:55:08 +0900\",\"categories\":null,\"tags\":\"NeuralNetworksAndDeepLearning\"}","markdownbody":"\n전 장에서 우리는 어떻게 뉴런 네트워크가 기울기 하강 알고리즘을 사용해서 가중치와 $bias$를 학습하는지에 대해 보았습니다. 그러나 여기에서 우리의 설명에는 큰 구멍이 하나 있었습니다: 우리는 어떻게 비용함수의 기울기를 계산하는지 이야기 하지 않았습니다. 정말 큰 구멍입니다! 이 장에서 저는 역전파 라고 알려진 그런 기울기를 계산하는 빠른 알고리즘에 대해서 설명할 것 입니다.\n\n역전파 알고리즘은 1970년대에 처음 소개되었지만, [데이비드 루멜하트][david_rumelhart], [제프리 힌튼][hinton], 그리고 [로널드 윌리엄스][ronald_williams]의 [유명한 1986년도의 논문][1986_paper]이 나오기 전 까지는 그 중요성이 인정되지 않았습니다. 이 논문은 이전에는 풀리지 않았던 문제들을 풀 수 있는 뉴런 네트워크의 사용을 가능케 하면서 이전의 학습에 대한 접근방법보다 역전파 알고리즘이 더 빨리 작동하는 몇개의 뉴런 네트워크에 대해 설명하였습니다. 오늘날, 역전파 알고리즘은 뉴런 네트워크에서의 학습의 대표주자가 되었습니다.\n\n\u003c!-- more --\u003e\n\n이 장은 이 책의 나머지 부분보다 수학적인 부분이 많이 연관되어 있습니다. 여러분이 수학에 미치지 않았다면 이 장을 넘겨버리고 역전파를 무시해버리고 싶은 내용들을 담은 판도라 상자처럼 여기고 싶을겁니다. 이런 내용들을 공부하는데 왜 시간을 투자해야 할까요?\n\n그 이유는, 당연히, 이해를 위한겁니다. 역전파의 핵심은 네트워크에서의 그 어떤 가중치 $w$(또는 bias $b$)에 대한 비용 함수 $C$의 편미분 $\\partial C/\\partial w$의 식입니다. 이 식은 우리에게 우리가 가중치와 $bias$를 바꿀때 비용함수가 얼마나 빠르게 바뀌는지에 대해 말해줍니다. 그리고 이 식이 좀 복잡한 반면, 각 항들은 자연적이고 직관적이 해석이 가능한 아름다움을 가지고 있습니다. 그리고 또한 역전파는 학습을 위한 단지 빠른 알고리즘이 아닙니다. 이는 가중치와 $bias$의 변화가 네트워크의 전체 행동을 변화시키는지에 대한 자세한 식견을 알려줍니다. 자세한 사항을 공부하는것은 정말 가치가 있습니다.\n\n앞서 말했듯이, 이 장을 넘기고 슥 지나가고 싶거나 그냥 바로 다음장으로 넘어가고 싶다면, 괜찮습니다. 저는 여러분이 역전파를 알수없는 판도라의 상자와 같이 다루더라도 책의 나머지 부분은 접근하기 쉽도록 작성하였으니까요. 물론 이 장으로 부터의 결론을 다시 언급하게될\u0026nbsp;책의 후반부에는 중요한 요점들이 있습니다. 그러나 그 중요한 요점들에서는 여러분이 모든 이유를 따라가지 못하더라도 주 결론에 대한 이해를 할 수 있어야 합니다.\n\n[david_rumelhart]: https://en.wikipedia.org/wiki/David_Rumelhart\n[hinton]: https://www.cs.toronto.edu/~hinton/\n[ronald_williams]: https://en.wikipedia.org/wiki/Ronald_J._Williams\n[1986_paper]: https://www.nature.com/nature/journal/v323/n6088/pdf/323533a0.pdf\n"},"__N_SSG":true},"page":"/blog/[...slug]","query":{"slug":["2017-04-18-neuralnet-chap2-how-the-backpropagation-algorithm-works"]},"buildId":"HQZBoNW2ycimZLlBm8Rpq","assetPrefix":"/blog","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/blog/_next/static/runtime/polyfills-b10afcedf826ebd862ad.js"></script><script async="" data-next-page="/_app" src="/blog/_next/static/HQZBoNW2ycimZLlBm8Rpq/pages/_app.js"></script><script async="" data-next-page="/blog/[...slug]" src="/blog/_next/static/HQZBoNW2ycimZLlBm8Rpq/pages/blog/%5B...slug%5D.js"></script><script src="/blog/_next/static/runtime/webpack-c212667a5f965e81e004.js" async=""></script><script src="/blog/_next/static/chunks/framework.619a4f70c1d4d3a29cbc.js" async=""></script><script src="/blog/_next/static/chunks/commons.020a96d8a8e71e9e3362.js" async=""></script><script src="/blog/_next/static/runtime/main-d76a4f4aadf0225115f4.js" async=""></script><script src="/blog/_next/static/chunks/93913148a803e02e94a06c77fa5869838dbed1a1.5637e4aab0992574c455.js" async=""></script><script src="/blog/_next/static/HQZBoNW2ycimZLlBm8Rpq/_buildManifest.js" async=""></script><script src="/blog/_next/static/HQZBoNW2ycimZLlBm8Rpq/_ssgManifest.js" async=""></script></body></html>