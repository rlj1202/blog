<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0" class="jsx-834064900"/><link rel="icon" href="/blog/favicon.ico" class="jsx-834064900"/><title class="jsx-1900133614">역전파에 대한 네가지 중요한 공식(작성중)</title><meta name="next-head-count" content="4"/><link rel="preload" href="/blog/_next/static/D0XTK0MEKRJDXLhC_InxX/pages/_app.js" as="script"/><link rel="preload" href="/blog/_next/static/D0XTK0MEKRJDXLhC_InxX/pages/blog/%5B...slug%5D.js" as="script"/><link rel="preload" href="/blog/_next/static/runtime/webpack-c212667a5f965e81e004.js" as="script"/><link rel="preload" href="/blog/_next/static/chunks/framework.619a4f70c1d4d3a29cbc.js" as="script"/><link rel="preload" href="/blog/_next/static/chunks/commons.020a96d8a8e71e9e3362.js" as="script"/><link rel="preload" href="/blog/_next/static/runtime/main-d76a4f4aadf0225115f4.js" as="script"/><link rel="preload" href="/blog/_next/static/chunks/e6dc419047dac61ef10415aaccc4c72c1149af9c.5637e4aab0992574c455.js" as="script"/><style id="__jsx-505471054">.top.jsx-1900133614{margin:0;background-color:#ff6565;box-shadow:0 0 5pt black;color:white;}.fixed-width.jsx-1900133614{max-width:600pt;min-width:0;padding:0 20pt;margin:0 auto;}.header.jsx-1900133614{padding-top:20pt;padding-bottom:20pt;}.header.jsx-1900133614 .navigator.jsx-1900133614{margin-bottom:20pt;}.header.jsx-1900133614 .navigator-link.jsx-1900133614{font-size:8pt;color:black;background-color:white;border-radius:16pt;padding:2pt 6pt;}.header.jsx-1900133614 .date.jsx-1900133614{margin:10pt 0;}.header.jsx-1900133614 .title.jsx-1900133614{font-size:35pt;font-weight:bold;margin:10pt 0;}.header.jsx-1900133614 .subtitle.jsx-1900133614{font-weight:bold;margin:10pt 0;}.tags.jsx-1900133614{margin-top:10pt;}.tag.jsx-1900133614{margin-right:4pt;padding:2pt 4pt;font-size:9pt;border-radius:3pt;background-color:#0000007a;display:inline-block;}.content-wrapper.jsx-1900133614{margin:20pt 0;}hr.jsx-1900133614{border:0;height:1px;background-color:#00000014;margin:20pt 0;}.prev-post.jsx-1900133614 .heading.jsx-1900133614,.next-post.jsx-1900133614 .heading.jsx-1900133614{margin-bottom:5pt;}.prev-post.jsx-1900133614 .title.jsx-1900133614,.next-post.jsx-1900133614 .title.jsx-1900133614{font-style:italic;color:#7d7d7d;}.next-post.jsx-1900133614{text-align:right;}.footer.jsx-1900133614{text-align:center;font-size:9pt;margin:20pt 0;}</style><style id="__jsx-1919476713">code{background-color:#dddddd;font-size:inherit;padding:2pt 4pt;border-radius:2pt;}.content img,.content iframe{max-width:100%;}</style><style id="__jsx-834064900">@import url(https://fonts.googleapis.com/earlyaccess/notosanskr.css);html,body{margin:0;padding:0;font-family:'Noto Sans KR',sans-serif;}a{color:inherit;-webkit-text-decoration:none;text-decoration:none;}*{box-sizing:border-box;}</style></head><body><div id="__next"><div class="jsx-834064900 container"><div class="jsx-1900133614 top"><div class="jsx-1900133614 header fixed-width"><div class="jsx-1900133614 navigator"><span class="jsx-1900133614 navigator-link"><a class="jsx-1900133614" href="/blog/">home</a></span> &gt;<span class="jsx-1900133614 navigator-link">blog</span></div><div class="jsx-1900133614 date">2017-04-18 19:57:43 +0900</div><div class="jsx-1900133614 title"><a class="jsx-1900133614" href="/blog/blog/2017-04-18-the-four-fundamental-equations-behind-backpropagation">역전파에 대한 네가지 중요한 공식(작성중)</a></div><div class="jsx-1900133614 subtitle"></div><div class="jsx-1900133614 tags"><span class="jsx-1900133614 tag">#<!-- -->NeuralNetworksAndDeepLearning</span></div></div></div><div class="jsx-1900133614 content-wrapper"><div class="jsx-1900133614 content fixed-width"><p>역전파는 어떻게 네트워크에서의 가중치와 $bias$의 변화가 비용함수를 바꾸는지에 대한 이해에 관한 것 입니다. 궁극적으로, 이는 $\partial C/\partial w^l<em>{jk}$ 그리고 $\partial C/\partial b^l_j$에 대한 편미분을 계산하는 것을 의미합니다. 그러나 이것을 계산하기 위해서 우리는 먼저 $l^{th}$ 층에 있는 $j^{th}$뉴런에서 error 라고 부르는 $\delta^l_j$라는 중간값을 소개하려고 합니다. 역전파는 error $\delta^l_j$를 계산하기 위한 과정을 말해주며 $\delta^l_j$를 $\partial C/\partial w^l</em>{jk}$와 $\partial C/\partial b^l_j$와 관련지어줍니다.</p><p>어떻게 error 가 정의되는지 이해하기 위해서, 우래의 네트워크에 악마가 하나 있다고 상상해 봅시다.</p><span><center><img src="/assets/neuralnet/tikz19.png" style="max-width:100%;height:auto"  height="240" width="583"/></center></span><span><!-- more --></span><p>$l$층에 있는 $j^{th}$ 뉴런에 악마가 있군요. 뉴런에 입력이 들어옴에 따라, 악마는 뉴런의 작동에 장난을 칠겁니다. 이 장난은 뉴런의 가중치 계산이 된 입력값에 작은 변화 $\Delta z^l_j$ 를 가할것이고, 뉴런은 $\sigma (z^l_j)$를 출력으로 내보내는 대신에 $\sigma (z^l_j + \Delta z^l_j)$를 내보낼 것 입니다. 이러한 변화는 네트워크의 나머지 층을 통해 전파되어, 결국에는 전체적인 비용에 대해 $\frac{\partial C}{\partial z^l_j} \Delta z^l_j$만큼의 변화를 만들어 낼 것입니다.</p><p>이제, 이 악마는 좋은 악마입니다. 그리고 비용을 향상시키기 위해 여러분에게 도움을 줄 것입니다. 악마는 비용을 작게 만들 수 있는 $\Delta z^l_j$를 찾으려고 노력할 것 입니다. $\frac{\partial C}{\partial z^l_j}$가 굉장히 큰 값을 가지고 있다고 가정해 봅시다(양수든 음수든). 그러면 악마는 $\frac{\partial C}{\partial z^l_j}$와는 반대의 부호를 가지는 $\Delta z^l_j$를 선택함으로써 비용함수의 값을 좀 낮출 수 있습니다. 반대로, $\frac{\partial C}{\partial z^l_j}$가 0에 가깝다면, 악마는 가중치가 계산된 $z^l_j$의 값을 통해 비용함수의 값을 개선할 수 없을 것 입니다. 그렇게 된다면 악마는 뉴런이 이미 꽤 잘 학습하였다고 말할 수 있습니다. 그리고 $\frac{\partial C}{\partial z^l_j}$가 뉴런의 error의 값의 척도라는 경험적인 것을 알 수 있군요.</p><p>이 이야기로부터 영감을 받아, 우리는 $l$ 층의 $j$ 뉴런의 error $\delta^l_j$를 다음과 같이 정의할 수 있습니다.</p><p>$$\begin{eqnarray}   \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}.\tag{29}\end{eqnarray}$$</p><hr class="jsx-1900133614"/><div class="jsx-1900133614 prev-post"><h2 class="jsx-1900133614 heading">이전글</h2><span class="jsx-1900133614 title">&quot;The test title&quot;</span></div><div class="jsx-1900133614 next-post"><h2 class="jsx-1900133614 heading">다음글</h2><span class="jsx-1900133614 title">&quot;Another post title&quot;</span></div><hr class="jsx-1900133614"/><footer class="jsx-1900133614 footer">Github.io - Copyright(c) 2020. Jisu Sim(RedLaboratory)</footer></div></div><style>
            .container {
            }
            </style></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"slug":["2017-04-18-the-four-fundamental-equations-behind-backpropagation"],"frontmatter":"{\"layout\":\"post\",\"title\":\"역전파에 대한 네가지 중요한 공식(작성중)\",\"date\":\"2017-04-18 19:57:43 +0900\",\"categories\":null,\"tags\":\"NeuralNetworksAndDeepLearning\"}","markdownbody":"역전파는 어떻게 네트워크에서의 가중치와 $bias$의 변화가 비용함수를 바꾸는지에 대한 이해에 관한 것 입니다. 궁극적으로, 이는 $\\partial C/\\partial w^l_{jk}$ 그리고 $\\partial C/\\partial b^l_j$에 대한 편미분을 계산하는 것을 의미합니다. 그러나 이것을 계산하기 위해서 우리는 먼저 $l^{th}$ 층에 있는 $j^{th}$뉴런에서 error 라고 부르는 $\\delta^l_j$라는 중간값을 소개하려고 합니다. 역전파는 error $\\delta^l_j$를 계산하기 위한 과정을 말해주며 $\\delta^l_j$를 $\\partial C/\\partial w^l_{jk}$와 $\\partial C/\\partial b^l_j$와 관련지어줍니다.\n\n어떻게 error 가 정의되는지 이해하기 위해서, 우래의 네트워크에 악마가 하나 있다고 상상해 봅시다.\n\n\u003ccenter\u003e\u003cimg src=\"/assets/neuralnet/tikz19.png\" style=\"max-width:100%;height:auto\"  height=\"240\" width=\"583\"/\u003e\u003c/center\u003e\n\n\u003c!-- more --\u003e\n\n$l$층에 있는 $j^{th}$ 뉴런에 악마가 있군요. 뉴런에 입력이 들어옴에 따라, 악마는 뉴런의 작동에 장난을 칠겁니다. 이 장난은 뉴런의 가중치 계산이 된 입력값에 작은 변화 $\\Delta z^l_j$ 를 가할것이고, 뉴런은 $\\sigma (z^l_j)$를 출력으로 내보내는 대신에 $\\sigma (z^l_j + \\Delta z^l_j)$를 내보낼 것 입니다. 이러한 변화는 네트워크의 나머지 층을 통해 전파되어, 결국에는 전체적인 비용에 대해 $\\frac{\\partial C}{\\partial z^l_j} \\Delta z^l_j$만큼의 변화를 만들어 낼 것입니다.\n\n이제, 이 악마는 좋은 악마입니다. 그리고 비용을 향상시키기 위해 여러분에게 도움을 줄 것입니다. 악마는 비용을 작게 만들 수 있는 $\\Delta z^l_j$를 찾으려고 노력할 것 입니다. $\\frac{\\partial C}{\\partial z^l_j}$가 굉장히 큰 값을 가지고 있다고 가정해 봅시다(양수든 음수든). 그러면 악마는 $\\frac{\\partial C}{\\partial z^l_j}$와는 반대의 부호를 가지는 $\\Delta z^l_j$를 선택함으로써 비용함수의 값을 좀 낮출 수 있습니다. 반대로, $\\frac{\\partial C}{\\partial z^l_j}$가 0에 가깝다면, 악마는 가중치가 계산된 $z^l_j$의 값을 통해 비용함수의 값을 개선할 수 없을 것 입니다. 그렇게 된다면 악마는 뉴런이 이미 꽤 잘 학습하였다고 말할 수 있습니다. 그리고 $\\frac{\\partial C}{\\partial z^l_j}$가 뉴런의 error의 값의 척도라는 경험적인 것을 알 수 있군요.\n\n이 이야기로부터 영감을 받아, 우리는 $l$ 층의 $j$ 뉴런의 error $\\delta^l_j$를 다음과 같이 정의할 수 있습니다.\n\n$$\\begin{eqnarray}   \\delta^l_j \\equiv \\frac{\\partial C}{\\partial z^l_j}.\\tag{29}\\end{eqnarray}$$\n"},"__N_SSG":true},"page":"/blog/[...slug]","query":{"slug":["2017-04-18-the-four-fundamental-equations-behind-backpropagation"]},"buildId":"D0XTK0MEKRJDXLhC_InxX","assetPrefix":"/blog","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/blog/_next/static/runtime/polyfills-b10afcedf826ebd862ad.js"></script><script async="" data-next-page="/_app" src="/blog/_next/static/D0XTK0MEKRJDXLhC_InxX/pages/_app.js"></script><script async="" data-next-page="/blog/[...slug]" src="/blog/_next/static/D0XTK0MEKRJDXLhC_InxX/pages/blog/%5B...slug%5D.js"></script><script src="/blog/_next/static/runtime/webpack-c212667a5f965e81e004.js" async=""></script><script src="/blog/_next/static/chunks/framework.619a4f70c1d4d3a29cbc.js" async=""></script><script src="/blog/_next/static/chunks/commons.020a96d8a8e71e9e3362.js" async=""></script><script src="/blog/_next/static/runtime/main-d76a4f4aadf0225115f4.js" async=""></script><script src="/blog/_next/static/chunks/e6dc419047dac61ef10415aaccc4c72c1149af9c.5637e4aab0992574c455.js" async=""></script><script src="/blog/_next/static/D0XTK0MEKRJDXLhC_InxX/_buildManifest.js" async=""></script><script src="/blog/_next/static/D0XTK0MEKRJDXLhC_InxX/_ssgManifest.js" async=""></script></body></html>